{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with keras and tensorflow\n",
    "\n",
    "N.B. You will need to pip install keras and tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we'll use sklearn's built-in breast cancer dataset. The next cell loads the data and prints the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data and initializing a Scaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
    "\n",
    "ss = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming our data\n",
    "\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34913849, -1.43851335, -0.41172595, -0.39047943, -1.86366229,\n",
       "        -1.26860704, -0.82617052, -0.95286585, -1.72936805, -0.9415409 ,\n",
       "        -0.86971355, -1.35865347, -0.83481506, -0.57230673, -0.74586846,\n",
       "        -0.65398319, -0.52583524, -0.94677147, -0.53781728, -0.63449458,\n",
       "        -0.54268486, -1.65565452, -0.58986401, -0.52555985, -1.51066925,\n",
       "        -0.89149994, -0.75021715, -0.91671059, -0.92508585, -0.80841115]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s[:1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing model and layer types\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Importing our optimizer\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/flatironschool/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/flatironschool/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "426/426 [==============================] - 0s 417us/step - loss: 1.5779 - val_loss: 1.5135\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.8630 - val_loss: 0.9340\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.5457 - val_loss: 0.6274\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.3950 - val_loss: 0.4651\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.3097 - val_loss: 0.3617\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.2514 - val_loss: 0.2972\n",
      "Epoch 7/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.2115 - val_loss: 0.2486\n",
      "Epoch 8/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.1791 - val_loss: 0.2125\n",
      "Epoch 9/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.1577 - val_loss: 0.1863\n",
      "Epoch 10/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.1385 - val_loss: 0.1669\n",
      "Epoch 11/100\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.1244 - val_loss: 0.1508\n",
      "Epoch 12/100\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.1129 - val_loss: 0.1394\n",
      "Epoch 13/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.1040 - val_loss: 0.1267\n",
      "Epoch 14/100\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0962 - val_loss: 0.1189\n",
      "Epoch 15/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0899 - val_loss: 0.1117\n",
      "Epoch 16/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0844 - val_loss: 0.1057\n",
      "Epoch 17/100\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0796 - val_loss: 0.1014\n",
      "Epoch 18/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0752 - val_loss: 0.0976\n",
      "Epoch 19/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0714 - val_loss: 0.0929\n",
      "Epoch 20/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0679 - val_loss: 0.0893\n",
      "Epoch 21/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0650 - val_loss: 0.0863\n",
      "Epoch 22/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0619 - val_loss: 0.0845\n",
      "Epoch 23/100\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0594 - val_loss: 0.0818\n",
      "Epoch 24/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0569 - val_loss: 0.0786\n",
      "Epoch 25/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0548 - val_loss: 0.0775\n",
      "Epoch 26/100\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0533 - val_loss: 0.0764\n",
      "Epoch 27/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0511 - val_loss: 0.0737\n",
      "Epoch 28/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0498 - val_loss: 0.0722\n",
      "Epoch 29/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0482 - val_loss: 0.0718\n",
      "Epoch 30/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "Epoch 31/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0455 - val_loss: 0.0698\n",
      "Epoch 32/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0443 - val_loss: 0.0683\n",
      "Epoch 33/100\n",
      "426/426 [==============================] - 0s 33us/step - loss: 0.0428 - val_loss: 0.0666\n",
      "Epoch 34/100\n",
      "426/426 [==============================] - 0s 32us/step - loss: 0.0420 - val_loss: 0.0673\n",
      "Epoch 35/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0408 - val_loss: 0.0659\n",
      "Epoch 36/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0397 - val_loss: 0.0663\n",
      "Epoch 37/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0393 - val_loss: 0.0654\n",
      "Epoch 38/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0380 - val_loss: 0.0642\n",
      "Epoch 39/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0373 - val_loss: 0.0643\n",
      "Epoch 40/100\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0365 - val_loss: 0.0637\n",
      "Epoch 41/100\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0359 - val_loss: 0.0633\n",
      "Epoch 42/100\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0352 - val_loss: 0.0623\n",
      "Epoch 43/100\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0344 - val_loss: 0.0623\n",
      "Epoch 44/100\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0337 - val_loss: 0.0624\n",
      "Epoch 45/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0334 - val_loss: 0.0618\n",
      "Epoch 46/100\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0327 - val_loss: 0.0611\n",
      "Epoch 47/100\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0322 - val_loss: 0.0613\n",
      "Epoch 48/100\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0318 - val_loss: 0.0611\n",
      "Epoch 49/100\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0313 - val_loss: 0.0605\n",
      "Epoch 50/100\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0308 - val_loss: 0.0604\n",
      "Epoch 51/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0303 - val_loss: 0.0588\n",
      "Epoch 52/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0299 - val_loss: 0.0600\n",
      "Epoch 53/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0297 - val_loss: 0.0599\n",
      "Epoch 54/100\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0291 - val_loss: 0.0588\n",
      "Epoch 55/100\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0285 - val_loss: 0.0588\n",
      "Epoch 56/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0281 - val_loss: 0.0589\n",
      "Epoch 57/100\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0279 - val_loss: 0.0583\n",
      "Epoch 58/100\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0274 - val_loss: 0.0581\n",
      "Epoch 59/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0269 - val_loss: 0.0580\n",
      "Epoch 60/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0267 - val_loss: 0.0587\n",
      "Epoch 61/100\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0267 - val_loss: 0.0575\n",
      "Epoch 62/100\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0261 - val_loss: 0.0577\n",
      "Epoch 63/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0257 - val_loss: 0.0567\n",
      "Epoch 64/100\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0254 - val_loss: 0.0572\n",
      "Epoch 65/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0250 - val_loss: 0.0558\n",
      "Epoch 66/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0247 - val_loss: 0.0568\n",
      "Epoch 67/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0246 - val_loss: 0.0555\n",
      "Epoch 68/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0244 - val_loss: 0.0560\n",
      "Epoch 69/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0240 - val_loss: 0.0549\n",
      "Epoch 70/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0238 - val_loss: 0.0553\n",
      "Epoch 71/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0236 - val_loss: 0.0548\n",
      "Epoch 72/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0233 - val_loss: 0.0537\n",
      "Epoch 73/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0227 - val_loss: 0.0539\n",
      "Epoch 74/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0224 - val_loss: 0.0540\n",
      "Epoch 75/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0225 - val_loss: 0.0541\n",
      "Epoch 76/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0219 - val_loss: 0.0528\n",
      "Epoch 77/100\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0220 - val_loss: 0.0534\n",
      "Epoch 78/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0221 - val_loss: 0.0532\n",
      "Epoch 79/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0216 - val_loss: 0.0539\n",
      "Epoch 80/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0213 - val_loss: 0.0525\n",
      "Epoch 81/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0209 - val_loss: 0.0520\n",
      "Epoch 82/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0205 - val_loss: 0.0527\n",
      "Epoch 83/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0207 - val_loss: 0.0518\n",
      "Epoch 84/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0202 - val_loss: 0.0517\n",
      "Epoch 85/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0201 - val_loss: 0.0520\n",
      "Epoch 86/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0197 - val_loss: 0.0513\n",
      "Epoch 87/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0197 - val_loss: 0.0509\n",
      "Epoch 88/100\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0194 - val_loss: 0.0508\n",
      "Epoch 89/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0194 - val_loss: 0.0510\n",
      "Epoch 90/100\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0193 - val_loss: 0.0507\n",
      "Epoch 91/100\n",
      "426/426 [==============================] - 0s 66us/step - loss: 0.0192 - val_loss: 0.0501\n",
      "Epoch 92/100\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0191 - val_loss: 0.0501\n",
      "Epoch 93/100\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.0187 - val_loss: 0.0499\n",
      "Epoch 94/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0186 - val_loss: 0.0491\n",
      "Epoch 95/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0183 - val_loss: 0.0499\n",
      "Epoch 96/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0181 - val_loss: 0.0492\n",
      "Epoch 97/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0179 - val_loss: 0.0493\n",
      "Epoch 98/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0179 - val_loss: 0.0497\n",
      "Epoch 99/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0181 - val_loss: 0.0501\n",
      "Epoch 100/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0175 - val_loss: 0.0500\n"
     ]
    }
   ],
   "source": [
    "# Constructing and compiling our model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "inputs = X_train_s.shape[1] # same number of input neurons as input features\n",
    "\n",
    "hiddens = inputs # same number of hidden neurons as input neurons\n",
    "\n",
    "model.add(Dense(hiddens, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "\n",
    "# Fitting our model\n",
    "\n",
    "model.fit(X_train_s, y_train, validation_data=(X_test_s, y_test), epochs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0174 - val_loss: 0.0491\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0173 - val_loss: 0.0489\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0171 - val_loss: 0.0492\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0168 - val_loss: 0.0479\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0166 - val_loss: 0.0483\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0172 - val_loss: 0.0483\n",
      "Epoch 7/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0172 - val_loss: 0.0488\n",
      "Epoch 8/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0171 - val_loss: 0.0482\n",
      "Epoch 9/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0167 - val_loss: 0.0478\n",
      "Epoch 10/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0164 - val_loss: 0.0474\n",
      "Epoch 11/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0159 - val_loss: 0.0470\n",
      "Epoch 12/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0159 - val_loss: 0.0467\n",
      "Epoch 13/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0156 - val_loss: 0.0467\n",
      "Epoch 14/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0158 - val_loss: 0.0462\n",
      "Epoch 15/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0469\n",
      "Epoch 16/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0154 - val_loss: 0.0469\n",
      "Epoch 17/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0161 - val_loss: 0.0461\n",
      "Epoch 18/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0148 - val_loss: 0.0471\n",
      "Epoch 19/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0154 - val_loss: 0.0457\n",
      "Epoch 20/100\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0150 - val_loss: 0.0462\n",
      "Epoch 21/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0146 - val_loss: 0.0452\n",
      "Epoch 22/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0144 - val_loss: 0.0453\n",
      "Epoch 23/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0142 - val_loss: 0.0457\n",
      "Epoch 24/100\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0146 - val_loss: 0.0446\n",
      "Epoch 25/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0141 - val_loss: 0.0455\n",
      "Epoch 26/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0141 - val_loss: 0.0452\n",
      "Epoch 27/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0142 - val_loss: 0.0444\n",
      "Epoch 28/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0140 - val_loss: 0.0443\n",
      "Epoch 29/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0138 - val_loss: 0.0439\n",
      "Epoch 30/100\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0141 - val_loss: 0.0440\n",
      "Epoch 31/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0135 - val_loss: 0.0447\n",
      "Epoch 32/100\n",
      "426/426 [==============================] - 0s 33us/step - loss: 0.0139 - val_loss: 0.0440\n",
      "Epoch 33/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0135 - val_loss: 0.0435\n",
      "Epoch 34/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0135 - val_loss: 0.0437\n",
      "Epoch 35/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0133 - val_loss: 0.0437\n",
      "Epoch 36/100\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0132 - val_loss: 0.0436\n",
      "Epoch 37/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0128 - val_loss: 0.0440\n",
      "Epoch 38/100\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0129 - val_loss: 0.0429\n",
      "Epoch 39/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0130 - val_loss: 0.0428\n",
      "Epoch 40/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 0.0429\n",
      "Epoch 41/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0125 - val_loss: 0.0427\n",
      "Epoch 42/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0124 - val_loss: 0.0427\n",
      "Epoch 43/100\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0123 - val_loss: 0.0427\n",
      "Epoch 44/100\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0125 - val_loss: 0.0424\n",
      "Epoch 45/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0123 - val_loss: 0.0428\n",
      "Epoch 46/100\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0125 - val_loss: 0.0424\n",
      "Epoch 47/100\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0123 - val_loss: 0.0425\n",
      "Epoch 48/100\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0122 - val_loss: 0.0417\n",
      "Epoch 49/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0126 - val_loss: 0.0419\n",
      "Epoch 50/100\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0121 - val_loss: 0.0425\n",
      "Epoch 51/100\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0126 - val_loss: 0.0421\n",
      "Epoch 52/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0125 - val_loss: 0.0421\n",
      "Epoch 53/100\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0118 - val_loss: 0.0414\n",
      "Epoch 54/100\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0119 - val_loss: 0.0411\n",
      "Epoch 55/100\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0116 - val_loss: 0.0409\n",
      "Epoch 56/100\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0119 - val_loss: 0.0407\n",
      "Epoch 57/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0117 - val_loss: 0.0407\n",
      "Epoch 58/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0112 - val_loss: 0.0406\n",
      "Epoch 59/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0112 - val_loss: 0.0414\n",
      "Epoch 60/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0113 - val_loss: 0.0404\n",
      "Epoch 61/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0113 - val_loss: 0.0416\n",
      "Epoch 62/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0113 - val_loss: 0.0408\n",
      "Epoch 63/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0109 - val_loss: 0.0402\n",
      "Epoch 64/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0110 - val_loss: 0.0401\n",
      "Epoch 65/100\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0109 - val_loss: 0.0405\n",
      "Epoch 66/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0107 - val_loss: 0.0398\n",
      "Epoch 67/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0106 - val_loss: 0.0404\n",
      "Epoch 68/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0107 - val_loss: 0.0406\n",
      "Epoch 69/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0106 - val_loss: 0.0406\n",
      "Epoch 70/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0108 - val_loss: 0.0403\n",
      "Epoch 71/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0104 - val_loss: 0.0395\n",
      "Epoch 72/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0103 - val_loss: 0.0397\n",
      "Epoch 73/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0105 - val_loss: 0.0402\n",
      "Epoch 74/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0105 - val_loss: 0.0409\n",
      "Epoch 75/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0106 - val_loss: 0.0403\n",
      "Epoch 76/100\n",
      "426/426 [==============================] - 0s 34us/step - loss: 0.0102 - val_loss: 0.0404\n",
      "Epoch 77/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0102 - val_loss: 0.0404\n",
      "Epoch 78/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0103 - val_loss: 0.0398\n",
      "Epoch 79/100\n",
      "426/426 [==============================] - 0s 34us/step - loss: 0.0101 - val_loss: 0.0397\n",
      "Epoch 80/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0100 - val_loss: 0.0403\n",
      "Epoch 81/100\n",
      "426/426 [==============================] - 0s 30us/step - loss: 0.0100 - val_loss: 0.0393\n",
      "Epoch 82/100\n",
      "426/426 [==============================] - 0s 32us/step - loss: 0.0097 - val_loss: 0.0394\n",
      "Epoch 83/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0097 - val_loss: 0.0396\n",
      "Epoch 84/100\n",
      "426/426 [==============================] - 0s 31us/step - loss: 0.0098 - val_loss: 0.0396\n",
      "Epoch 85/100\n",
      "426/426 [==============================] - 0s 32us/step - loss: 0.0100 - val_loss: 0.0401\n",
      "Epoch 86/100\n",
      "426/426 [==============================] - 0s 32us/step - loss: 0.0098 - val_loss: 0.0397\n",
      "Epoch 87/100\n",
      "426/426 [==============================] - 0s 32us/step - loss: 0.0100 - val_loss: 0.0398\n",
      "Epoch 88/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0094 - val_loss: 0.0400\n",
      "Epoch 89/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0099 - val_loss: 0.0410\n",
      "Epoch 90/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0095 - val_loss: 0.0396\n",
      "Epoch 91/100\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0095 - val_loss: 0.0396\n",
      "Epoch 92/100\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0093 - val_loss: 0.0397\n",
      "Epoch 93/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0097 - val_loss: 0.0398\n",
      "Epoch 94/100\n",
      "426/426 [==============================] - 0s 35us/step - loss: 0.0092 - val_loss: 0.0388\n",
      "Epoch 95/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0089 - val_loss: 0.0397\n",
      "Epoch 96/100\n",
      "426/426 [==============================] - 0s 34us/step - loss: 0.0090 - val_loss: 0.0392\n",
      "Epoch 97/100\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0091 - val_loss: 0.0397\n",
      "Epoch 98/100\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0091 - val_loss: 0.0389\n",
      "Epoch 99/100\n",
      "426/426 [==============================] - 0s 34us/step - loss: 0.0088 - val_loss: 0.0389\n",
      "Epoch 100/100\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0089 - val_loss: 0.0384\n"
     ]
    }
   ],
   "source": [
    "# Storing that fit as a history log\n",
    "\n",
    "history = model.fit(X_train_s, y_train, validation_data=(X_test_s, y_test), epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our losses\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing loss')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow as a graph constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the graph\n",
    "\n",
    "a = tf.Variable(3)\n",
    "b = tf.Variable(10)\n",
    "\n",
    "c = a + b\n",
    "d = a + c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    result = session.run(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "# Printing the output\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 30))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "hid = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "y_hat = tf.layers.dense(hid, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790209790209791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train_s, y: y_train.reshape(-1, 1)})\n",
    "        \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test_s})\n",
    "\n",
    "classes = (pred > 0.5).astype(int)\n",
    "\n",
    "metrics.accuracy_score(y_test.reshape(-1, 1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
